{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058577f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import os\n",
    "from matplotlib.pyplot import hist\n",
    "# import more functions or modules if you need them !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774f543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for numpy\n",
    "RANDOM_SEED=42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53433a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df = pd.read_pickle('../Data/data/comprehensive_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97694e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onset2COWCS</th>\n",
       "      <th>onsetUCS</th>\n",
       "      <th>coup</th>\n",
       "      <th>periregular</th>\n",
       "      <th>numcode</th>\n",
       "      <th>year</th>\n",
       "      <th>ecgrowth</th>\n",
       "      <th>logmountain</th>\n",
       "      <th>ethnic_fractionalization</th>\n",
       "      <th>religion_fractionalization</th>\n",
       "      <th>...</th>\n",
       "      <th>valoilres_binarize</th>\n",
       "      <th>valoilres_public_diff</th>\n",
       "      <th>valoilres_public_binarize</th>\n",
       "      <th>oilpop_diff</th>\n",
       "      <th>oilpop_binarize</th>\n",
       "      <th>valoilres_impute_diff</th>\n",
       "      <th>valoilres_impute_binarize</th>\n",
       "      <th>oilpop_impute_diff</th>\n",
       "      <th>oilpop_impute_binarize</th>\n",
       "      <th>milexp_pergdpSIPRI_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17915</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>894</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>0.038210</td>\n",
       "      <td>-0.016094</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17916</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>894</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>0.038433</td>\n",
       "      <td>-0.016094</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17917</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>894</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>-0.016094</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17918</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>894</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.045470</td>\n",
       "      <td>-0.016094</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17919</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>894</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.040556</td>\n",
       "      <td>-0.016094</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17920 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       onset2COWCS  onsetUCS     coup  periregular  numcode    year  ecgrowth  \\\n",
       "0          0.00001       NaN      NaN          NaN        4  1929.0       NaN   \n",
       "1          0.00001       NaN      NaN      0.00001        4  1930.0       NaN   \n",
       "2          0.00001       NaN      NaN      0.00001        4  1931.0       NaN   \n",
       "3          0.00001       NaN      NaN      0.00001        4  1932.0       NaN   \n",
       "4          0.00001       NaN      NaN      0.00001        4  1933.0       NaN   \n",
       "...            ...       ...      ...          ...      ...     ...       ...   \n",
       "17915      0.00001   0.00001  0.00001          NaN      894  2004.0  0.038210   \n",
       "17916      0.00001   0.00001  0.00001          NaN      894  2005.0  0.038433   \n",
       "17917          NaN   0.00001  0.00001          NaN      894  2006.0  0.045788   \n",
       "17918          NaN   0.00001  0.00001          NaN      894  2007.0  0.045470   \n",
       "17919          NaN   0.00001  0.00001          NaN      894  2008.0  0.040556   \n",
       "\n",
       "       logmountain  ethnic_fractionalization  religion_fractionalization  ...  \\\n",
       "0         0.041836                  0.007693                    0.002717  ...   \n",
       "1         0.041836                  0.007693                    0.002717  ...   \n",
       "2         0.041836                  0.007693                    0.002717  ...   \n",
       "3         0.041836                  0.007693                    0.002717  ...   \n",
       "4         0.041836                  0.007693                    0.002717  ...   \n",
       "...            ...                       ...                         ...  ...   \n",
       "17915    -0.016094                  0.007808                    0.007359  ...   \n",
       "17916    -0.016094                  0.007808                    0.007359  ...   \n",
       "17917    -0.016094                  0.007808                    0.007359  ...   \n",
       "17918    -0.016094                  0.007808                    0.007359  ...   \n",
       "17919    -0.016094                  0.007808                    0.007359  ...   \n",
       "\n",
       "       valoilres_binarize  valoilres_public_diff  valoilres_public_binarize  \\\n",
       "0                     NaN                    NaN                        NaN   \n",
       "1                     NaN                    NaN                        NaN   \n",
       "2                     NaN                    NaN                        NaN   \n",
       "3                     NaN                    NaN                        NaN   \n",
       "4                     NaN                    NaN                        NaN   \n",
       "...                   ...                    ...                        ...   \n",
       "17915                 0.0                    0.0                        0.0   \n",
       "17916                 0.0                    0.0                        0.0   \n",
       "17917                 0.0                    0.0                        0.0   \n",
       "17918                 0.0                    0.0                        0.0   \n",
       "17919                 0.0                    0.0                        0.0   \n",
       "\n",
       "       oilpop_diff  oilpop_binarize  valoilres_impute_diff  \\\n",
       "0              NaN              NaN                    NaN   \n",
       "1              NaN              NaN                    NaN   \n",
       "2              NaN              NaN                    NaN   \n",
       "3              NaN              NaN                    NaN   \n",
       "4              NaN              NaN                    NaN   \n",
       "...            ...              ...                    ...   \n",
       "17915          0.0              0.0                    0.0   \n",
       "17916          0.0              0.0                    0.0   \n",
       "17917          0.0              0.0                    0.0   \n",
       "17918          0.0              0.0                    0.0   \n",
       "17919          0.0              0.0                    0.0   \n",
       "\n",
       "       valoilres_impute_binarize  oilpop_impute_diff  oilpop_impute_binarize  \\\n",
       "0                            NaN                 NaN                     NaN   \n",
       "1                            NaN                 NaN                     NaN   \n",
       "2                            NaN                 NaN                     NaN   \n",
       "3                            NaN                 NaN                     NaN   \n",
       "4                            NaN                 NaN                     NaN   \n",
       "...                          ...                 ...                     ...   \n",
       "17915                        0.0                 0.0                     0.0   \n",
       "17916                        0.0                 0.0                     0.0   \n",
       "17917                        0.0                 0.0                     0.0   \n",
       "17918                        0.0                 0.0                     0.0   \n",
       "17919                        0.0                 0.0                     0.0   \n",
       "\n",
       "       milexp_pergdpSIPRI_diff  \n",
       "0                          NaN  \n",
       "1                          NaN  \n",
       "2                          NaN  \n",
       "3                          NaN  \n",
       "4                          NaN  \n",
       "...                        ...  \n",
       "17915                      NaN  \n",
       "17916                      NaN  \n",
       "17917                      NaN  \n",
       "17918                      NaN  \n",
       "17919                      NaN  \n",
       "\n",
       "[17920 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b0d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATT and ATE AIPTW\n",
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "    \"\"\"\n",
    "    Double ML estimator for the ATT\n",
    "    This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "    Return: aiptw of ATE and its standard error\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of observations\n",
    "    n = Y.shape[0]\n",
    "    \n",
    "    # estimate marginal probability of treatment\n",
    "    if prob_t is None:\n",
    "        prob_t = A.mean() \n",
    "    \n",
    "    # att aiptw\n",
    "    tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "    # influence curve and standard error of aiptw\n",
    "    phi = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "    std_hat = np.std(phi) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, std_hat\n",
    "\n",
    "def ate_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "    \"\"\"\n",
    "    Double ML estimator for the ATE\n",
    "    Return: aiptw of ATE and its standard error\n",
    "    \"\"\"\n",
    "    # number of observations\n",
    "    n = Y.shape[0]\n",
    "    \n",
    "    # ate aiptw\n",
    "    tau_hat = (Q1 - Q0 + A*(Y-Q1)/g - (1-A)*(Y-Q0)/(1-g)).mean()\n",
    "  \n",
    "    # influence curve and standard error of aiptw\n",
    "    phi = Q1 - Q0 + A*(Y-Q1)/g - (1-A)*(Y-Q0)/(1-g) - tau_hat   \n",
    "    std_hat = np.std(phi) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, std_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be19d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional outcome models (Q models)\n",
    "def make_linear_Q_model():\n",
    "    ''' A function that returns a linear q model for later use in k-folding'''\n",
    "    return LinearRegression()\n",
    "\n",
    "def make_Q_model(output_type:str):\n",
    "    ''' A function that returns a general ML q model for later use in k-folding'''\n",
    "    if output_type == 'binary':\n",
    "        return RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=500, max_depth=None)\n",
    "    return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=500, max_depth=None)\n",
    "# One example: RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=500, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf517f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propensity score models (g models)\n",
    "def make_g_model():\n",
    "    ''' A function that returns a g model for computing propensity scores'''\n",
    "    return RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "# One example: RandomForestClassifier(n_estimators=100, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52dee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for K-fold cross-fitting\n",
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    '''\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    '''\n",
    "\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "        X_train = X.loc[train_index]\n",
    "        A_train = A.loc[train_index]\n",
    "        g = make_model()\n",
    "        g.fit(X_train, A_train)\n",
    "\n",
    "        # get predictions for split\n",
    "        predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "    \n",
    "    # sanity check that overlap holds\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
    "    '''\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
    "    '''\n",
    "\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
    "    if output_type == 'binary':\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    elif output_type == 'continuous':\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # for predicting effect under treatment / control status for each data point \n",
    "    X0 = X_w_treatment.copy()\n",
    "    X0[\"A\"] = 0\n",
    "    X1 = X_w_treatment.copy()\n",
    "    X1[\"A\"] = 1\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "        X_train = X_w_treatment.loc[train_index]\n",
    "        y_train = y.loc[train_index]\n",
    "        q = make_model(output_type)\n",
    "        q.fit(X_train, y_train)\n",
    "\n",
    "        if output_type =='binary':\n",
    "            predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "            predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "        elif output_type == 'continuous':\n",
    "            predictions0[test_index] = q.predict(X0.loc[test_index])\n",
    "            predictions1[test_index] = q.predict(X1.loc[test_index])\n",
    "\n",
    "    assert np.isnan(predictions0).sum() == 0\n",
    "    assert np.isnan(predictions1).sum() == 0\n",
    "    return predictions0, predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "120e881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars = ['onset2COWCS',\n",
    "              'valoilres_binarize',\n",
    "              'ecgrowth',\n",
    "              'pop_maddison_diff',\n",
    "              'popdens_diff',\n",
    "              'democracy_diff',\n",
    "              'logmountain',\n",
    "              'ethnic_fractionalization',\n",
    "              'religion_fractionalization',\n",
    "              'language_fractionalization',\n",
    "              'leg_british']\n",
    "              #'numcode',\n",
    "              #'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f9a4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'onset2COWCS'\n",
    "treatment = 'valoilres_binarize'\n",
    "confounders = [x for x in model_vars if x not in (outcome + treatment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "129b3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_run_model(df, outcome:str, treatment:str, confounders:list, make_g_model,\n",
    "                      make_Q_model, n_splits=5, output_type='binary'):\n",
    "    '''\n",
    "    Function that creates a g, q, and aiptw model based on the \n",
    "    given inputs\n",
    "    \n",
    "    Inputs: df (pandas df) - the dataframe the variables are contained in\n",
    "            outcome (str) - the outcome variable\n",
    "            treatment (str) - the treatment variable\n",
    "            confounders (lst) - a list of the confounding variables\n",
    "            make_g_model - the make_g_model function\n",
    "            make_Q_model - the make_Q_model function\n",
    "            n_splits (int) - number of splits for the model\n",
    "            output_type (str) - the desired output type, either binary or continous\n",
    "    \n",
    "    Returns: tau_hat - the tau hat estimator for the average treatment effect\n",
    "             std of tau_hat - the standard deviation for the tau_hat estimator\n",
    "    '''\n",
    "    print('Running models for treatment {} and outcome {}'.format(treatment, outcome))\n",
    "    df = df.replace({outcome: .00001}, 0)\n",
    "    df = df[[outcome] + confounders + [treatment]]\n",
    "    df = df.dropna().reset_index()\n",
    "    outcome = df[outcome]\n",
    "    confounders = df[confounders]\n",
    "    treatment = df[treatment]\n",
    "    \n",
    "    g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=n_splits)\n",
    "    \n",
    "    if min(g) < .01:\n",
    "        print('\\nWARNING:\\n Some propensity scores are very small,\\n which could '\n",
    "              'lead to an inflated AIPTW.\\n Minimum score = ', min(g))\n",
    "    if max(g) > .99:\n",
    "        print('\\nWARNING:\\n Some propensity scores are very large,\\n which could '\n",
    "              'lead to an inflated AIPTW.\\n Maximum score = ', max(g))\n",
    "    print('G Model has been fit')\n",
    "\n",
    "    Q0_ml, Q1_ml = outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, \\\n",
    "                                                  n_splits=n_splits, output_type=output_type)\n",
    "    \n",
    "    print('Q model has been fit')\n",
    "    data_and_nuisance_estimates_ml = pd.DataFrame({'g': g, 'Q0': Q0_ml, 'Q1': Q1_ml, 'A': treatment, 'Y': outcome})\n",
    "    \n",
    "    # ate aiptw\n",
    "    tau_hat, std_hat = ate_aiptw(**data_and_nuisance_estimates_ml)\n",
    "    print('AIPTW model has been fit. Returning \\u03C4 hat and its standard deviation')\n",
    "    print('\\u03C4 hat = {} and std = {}\\n'.format(round(tau_hat, 5), round(std_hat, 5)))\n",
    "    return tau_hat, std_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b70b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_lst = ['valoilres_binarize', # value of oil reserves\n",
    "               'valoilres_public_binarize', # value of oil reserves from public data\n",
    "               'oilpop_binarize', # oil reserves per capita in million barrels per 1000 persons\n",
    "               'valoilres_impute_binarize', # value of oilpop_impute (multiply by crude oil price)\n",
    "               'oilpop_impute_binarize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87eec96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models for treatment valoilres_binarize and outcome onset2COWCS\n",
      "G Model has been fit\n",
      "Q model has been fit\n",
      "AIPTW model has been fit. Returning τ hat and its standard deviation\n",
      "τ = 0.00344 and std = 0.00442\n",
      "\n",
      "Running models for treatment valoilres_public_binarize and outcome onset2COWCS\n",
      "G Model has been fit\n",
      "Q model has been fit\n",
      "AIPTW model has been fit. Returning τ hat and its standard deviation\n",
      "τ = 0.00349 and std = 0.00434\n",
      "\n",
      "Running models for treatment oilpop_binarize and outcome onset2COWCS\n",
      "\n",
      "WARNING:\n",
      " Some propensity scores are very small,\n",
      " which couldlead to an inflated AIPTW.\n",
      " Minimum score =  0.0035422188315889338\n",
      "G Model has been fit\n",
      "Q model has been fit\n",
      "AIPTW model has been fit. Returning τ hat and its standard deviation\n",
      "τ = -0.0036 and std = 0.00605\n",
      "\n",
      "Running models for treatment valoilres_impute_binarize and outcome onset2COWCS\n",
      "G Model has been fit\n",
      "Q model has been fit\n",
      "AIPTW model has been fit. Returning τ hat and its standard deviation\n",
      "τ = 0.00404 and std = 0.00443\n",
      "\n",
      "Running models for treatment oilpop_impute_binarize and outcome onset2COWCS\n",
      "\n",
      "WARNING:\n",
      " Some propensity scores are very small,\n",
      " which couldlead to an inflated AIPTW.\n",
      " Minimum score =  0.004493974015149532\n",
      "G Model has been fit\n",
      "Q model has been fit\n",
      "AIPTW model has been fit. Returning τ hat and its standard deviation\n",
      "τ = -0.00435 and std = 0.00542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for treat in treatment_lst:\n",
    "    fit_and_run_model(oil_df, outcome, treat, confounders, make_g_model, make_Q_model, n_splits=5, output_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3216025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
